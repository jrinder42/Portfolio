{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import gzip # doesn't work for this data\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import SGDClassifier \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the file exist in the working directory? True\n"
     ]
    }
   ],
   "source": [
    "exist = os.path.isfile('reddit_submissions.json.gz')\n",
    "print('Does the file exist in the working directory?', exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subreddit multi-class classification\n",
    "d = pd.read_json('reddit_submissions.json.gz', lines=True, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the data (944088, 10)\n",
      "         author  created_utc     id  num_comments selftext        subreddit  \\\n",
      "0         Crito   1223866465  76r64             0                      ptsd   \n",
      "1         Crito   1228150531  7goht             0                      ptsd   \n",
      "2  socialcelebs   1228219003  7guki             0              mentalhealth   \n",
      "3     [deleted]   1228244460  7gxll             1           EatingDisorders   \n",
      "4     [deleted]   1228244538  7gxm3             1           EatingDisorders   \n",
      "5     [deleted]   1228244714  7gxn0             1           EatingDisorders   \n",
      "6     [deleted]   1228244974  7gxnq             0           EatingDisorders   \n",
      "7     [deleted]   1228245123  7gxof             0           EatingDisorders   \n",
      "8     [deleted]   1228245234  7gxot             0           EatingDisorders   \n",
      "9     [deleted]   1228245285  7gxp4             0           EatingDisorders   \n",
      "\n",
      "  subreddit_name_prefixed subreddit_type  \\\n",
      "0                  r/ptsd         public   \n",
      "1                  r/ptsd         public   \n",
      "2          r/mentalhealth         public   \n",
      "3       r/EatingDisorders     restricted   \n",
      "4       r/EatingDisorders     restricted   \n",
      "5       r/EatingDisorders     restricted   \n",
      "6       r/EatingDisorders     restricted   \n",
      "7       r/EatingDisorders     restricted   \n",
      "8       r/EatingDisorders     restricted   \n",
      "9       r/EatingDisorders     restricted   \n",
      "\n",
      "                                               title  \\\n",
      "0  A plea for the wider victims of traumatic stress.   \n",
      "1                       Cortisone may help ease PTSD   \n",
      "2                 Eliminate Anxiety Attacks for Good   \n",
      "3                    Bulimia just gets on my nerves!   \n",
      "4              Why throwing up makes you GAIN weight   \n",
      "5                         Bulimia Dental Safety tips   \n",
      "6  Why this brave girl can only eat tic tacs ( Ke...   \n",
      "7                    7 Lesser-Known Eating Disorders   \n",
      "8  Celebrities: Famous People who died or have Ea...   \n",
      "9  Using text messaging in the treatment of eatin...   \n",
      "\n",
      "                                                 url  \n",
      "0  http://www.jpost.com/servlet/Satellite?cid=122...  \n",
      "1  http://www.jpost.com/servlet/Satellite?cid=122...  \n",
      "2  http://www.relief-anxiety.info/eliminate-anxie...  \n",
      "3  http://operationlola.wordpress.com/2008/11/30/...  \n",
      "4  http://operationlola.wordpress.com/2008/11/28/...  \n",
      "5  http://operationlola.wordpress.com/2008/11/20/...  \n",
      "6  http://www.kentonline.co.uk/kol08/article/defa...  \n",
      "7  http://www.casapalmera.com/articles/7-lesser-k...  \n",
      "8  http://www.edreferral.com/Celebrities_who_died...  \n",
      "9  http://www.nursingtimes.net/ntclinical/2008/10...  \n"
     ]
    }
   ],
   "source": [
    "print('shape of the data', d.shape)\n",
    "print(d.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all subreddit categories and counts Counter({'depression': 489930, 'SuicideWatch': 182855, 'mentalhealth': 53965, 'BPD': 45630, 'socialanxiety': 42566, 'BipolarReddit': 22245, 'schizophrenia': 21549, 'MMFB': 17958, 'alcoholism': 12916, 'ptsd': 11063, 'rapecounseling': 9478, 'dpdr': 7222, 'StopSelfHarm': 6292, 'getting_over_it': 6057, 'Anger': 4772, 'EatingDisorders': 2964, 'survivorsofabuse': 2612, 'feelgood': 1554, 'hardshipmates': 1207, 'psychoticreddit': 549, 'PanicParty': 460, 'traumatoolbox': 244})\n",
      "\n",
      "subreddit categories with fewer than 1000 examples ['PanicParty', 'psychoticreddit', 'traumatoolbox']\n"
     ]
    }
   ],
   "source": [
    "# frequency of subreddit categories - for now we will leave it as is, but this might change\n",
    "c = Counter(d['subreddit'])\n",
    "fewer = [sub for sub in c if c[sub] < 1000]        \n",
    "print('all subreddit categories and counts', c)\n",
    "print()\n",
    "print('subreddit categories with fewer than 1000 examples', fewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elements to remove from data\n",
      "blank 67614 | removed 30715 | deleted 151654 | fraction 0.26478781638999754\n"
     ]
    }
   ],
   "source": [
    "# This is for the 'selftext' column\n",
    "\n",
    "# count how many elements are in data that we have to drop \n",
    "st = d['selftext']\n",
    "blank = st[st == '']\n",
    "removed = st[st == '[removed]']\n",
    "deleted = st[st == '[deleted]']\n",
    "total = len(blank) + len(removed) + len(deleted)\n",
    "print('elements to remove from data')\n",
    "print('blank', len(blank), '|', 'removed', len(removed), '|', 'deleted', len(deleted), '|', 'fraction', total / len(st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new shape of data after selftext deleting (694105, 10)\n"
     ]
    }
   ],
   "source": [
    "# deleting selftext data\n",
    "d = d[~d.selftext.isin(['', '[removed]', '[deleted]'])]\n",
    "print('new shape of data after selftext deleting', d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elements to remove from data\n",
      "blank 0 | removed 0 | deleted 0 | fraction 0.0\n",
      "Did our data processing work? True\n"
     ]
    }
   ],
   "source": [
    "# check if our dropping process was successful - we should get 0 for each case\n",
    "st = d['selftext']\n",
    "blank = st[st == '']\n",
    "removed = st[st == '[removed]']\n",
    "deleted = st[st == '[deleted]']\n",
    "total = len(blank) + len(removed) + len(deleted)\n",
    "print('elements to remove from data')\n",
    "print('blank', len(blank), '|', 'removed', len(removed), '|', 'deleted', len(deleted), '|', 'fraction', total / len(st))\n",
    "print('Did our data processing work?', total == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check that the correct rows were deleted from the data. This should be reflected in the reduced sample size from 944088 to 694105. Now we have to also clean the 'num_comments' column. We want to make sure that there are a sufficient number of comments for a story (>=5) so that the row can positively contribute to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of data that have less than 5 comments 0.6589248024434343\n"
     ]
    }
   ],
   "source": [
    "# Now we will clean up the 'num_comments' column so that the rows all have >= 5 comments\n",
    "print('fraction of data that have less than 5 comments', len(d[d.num_comments < 5]) / len(d.num_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new shape of data after deleting num_comments data (236742, 10)\n"
     ]
    }
   ],
   "source": [
    "# deleting the appropriate num_comments data\n",
    "d = d[d.num_comments >= 5]\n",
    "print('new shape of data after deleting num_comments data', d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of data that have less than 5 comments after deleting 0.0\n"
     ]
    }
   ],
   "source": [
    "# check if our dropping process was successful - we should get 0 \n",
    "print('fraction of data that have less than 5 comments after deleting', len(d[d.num_comments < 5]) / len(d.num_comments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like before, the sample size has become even smaller. The data went from 694105 to 236742. Our final preprocessing task is to figure out what to do with the subreddit categories that have fewer that 1000 examples. Since it does not make sense to group those categories into other larger ones, we should just leave the subreddit categories the way they are. The main concern with doing this is that it might lead to classifications issues given the small sample size for the 3 subreddit categories mentioned above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Partitioning the data into training, validation, and testing sets (60/20/20 split)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = d[['title', 'selftext']]\n",
    "y = d['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5) # random_state is for reproducibility\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning the data is rather straight forward. We simply split the data such that 60% is for training, 20% is for validation, and the remaining 20% is for testing. The validation set is used to pick which model we will use. The testing set is only used at the very end to get the out-of-sample performance. To ensure reproducibility, we add a random seed value (5). Now we will get the same results no matter how many times we run this code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Model Training: Multi-Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X variables: \n",
    "1. title\n",
    "2. selftext\n",
    "\n",
    "Y variables:\n",
    "1. subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models we will use:**\n",
    "* Naive Bayes\n",
    "* Stochastic Gradient Descent Classifier\n",
    "* Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to fit and transform our text data when we vectorize it\n",
    "\n",
    "# Fit and Transform for Text\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field):\n",
    "        self.field = field\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.field]\n",
    "\n",
    "# Fit and Transform for Numerical\n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field):\n",
    "        self.field = field\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[[self.field]]\n",
    "    \n",
    "# Tokenize based on Snowball Stemmer (Porter 2)\n",
    "def Tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    snowball_stemmer = nltk.SnowballStemmer('english', ignore_stopwords=True)\n",
    "    words = [snowball_stemmer.stem(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Naive Bayes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Naive Bayes Pipeline using FeatureUnion for our 2 predictor variables\n",
    "nb_classifier = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('title', Pipeline([('text', TextSelector('title')),\n",
    "                            ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, stop_words=stopwords.words('english')))]\n",
    "        )),\n",
    "        ('selftext', Pipeline([('text', TextSelector('selftext')),\n",
    "                               ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, stop_words=stopwords.words('english')))]\n",
    "        ))\n",
    "    ])),\n",
    "    ('clf', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = nb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Success Rate 0.46722458354233215\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      " EatingDisorders       0.00      0.00      0.00       187\n",
      "   hardshipmates       0.90      0.14      0.24      2639\n",
      "      depression       0.78      0.20      0.32      2223\n",
      "        feelgood       1.00      0.01      0.01       200\n",
      "      alcoholism       0.67      0.00      0.00      1063\n",
      "   BipolarReddit       0.00      0.00      0.00        15\n",
      "            dpdr       0.00      0.00      0.00       315\n",
      " getting_over_it       0.70      0.32      0.44      8802\n",
      "   socialanxiety       1.00      0.02      0.03       761\n",
      "            ptsd       0.42      0.94      0.58     14731\n",
      "           Anger       1.00      0.01      0.03       429\n",
      "  rapecounseling       0.00      0.00      0.00         2\n",
      " psychoticreddit       0.00      0.00      0.00       315\n",
      "            MMFB       0.00      0.00      0.00        32\n",
      "    StopSelfHarm       0.00      0.00      0.00      1179\n",
      "      PanicParty       0.00      0.00      0.00        15\n",
      "    SuicideWatch       0.75      0.00      0.01       710\n",
      "    mentalhealth       0.00      0.00      0.00       446\n",
      "   traumatoolbox       0.92      0.09      0.17      1697\n",
      "survivorsofabuse       0.92      0.03      0.05      1989\n",
      "             BPD       0.00      0.00      0.00       126\n",
      "   schizophrenia       0.00      0.00      0.00         3\n",
      "\n",
      "       micro avg       0.47      0.47      0.47     37879\n",
      "       macro avg       0.41      0.08      0.09     37879\n",
      "    weighted avg       0.60      0.47      0.38     37879\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_predict = nb_classifier.predict(X_val)\n",
    "nb_success = np.mean(nb_predict == y_val)\n",
    "print('Naive Bayes Success Rate', nb_success)\n",
    "print(classification_report(y_val, nb_predict, target_names=list(set(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stochastic Gradient Descent Classification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Stochastic Gradient Descent Classification Pipeline using FeatureUnion for our 2 predictor variables\n",
    "sgd_classifier = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('title', Pipeline([('text', TextSelector('title')),\n",
    "                            ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, stop_words=stopwords.words('english')))]\n",
    "        )),\n",
    "        ('selftext', Pipeline([('text', TextSelector('selftext')),\n",
    "                               ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, stop_words=stopwords.words('english')))]\n",
    "        ))\n",
    "    ])),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, max_iter=5, random_state=5)) # Ridge Regression\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_classifier = sgd_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent Classification Success Rate 0.6502811584255128\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      " EatingDisorders       0.76      0.35      0.48       187\n",
      "   hardshipmates       0.86      0.54      0.66      2639\n",
      "      depression       0.77      0.53      0.63      2223\n",
      "        feelgood       0.85      0.94      0.89       200\n",
      "      alcoholism       0.91      0.18      0.30      1063\n",
      "   BipolarReddit       0.60      0.20      0.30        15\n",
      "            dpdr       0.63      0.22      0.33       315\n",
      " getting_over_it       0.60      0.72      0.65      8802\n",
      "   socialanxiety       0.75      0.86      0.80       761\n",
      "            ptsd       0.61      0.78      0.69     14731\n",
      "           Anger       0.90      0.58      0.71       429\n",
      "  rapecounseling       0.00      0.00      0.00         2\n",
      " psychoticreddit       0.00      0.00      0.00       315\n",
      "            MMFB       0.67      0.06      0.11        32\n",
      "    StopSelfHarm       0.58      0.05      0.09      1179\n",
      "      PanicParty       0.00      0.00      0.00        15\n",
      "    SuicideWatch       0.80      0.55      0.65       710\n",
      "    mentalhealth       0.73      0.68      0.71       446\n",
      "   traumatoolbox       0.78      0.57      0.66      1697\n",
      "survivorsofabuse       0.72      0.57      0.64      1989\n",
      "             BPD       0.60      0.07      0.13       126\n",
      "   schizophrenia       0.00      0.00      0.00         3\n",
      "\n",
      "       micro avg       0.65      0.65      0.65     37879\n",
      "       macro avg       0.60      0.38      0.43     37879\n",
      "    weighted avg       0.67      0.65      0.63     37879\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd_predict = sgd_classifier.predict(X_val)\n",
    "sgd_success = np.mean(sgd_predict == y_val)\n",
    "print('Stochastic Gradient Descent Classification Success Rate', sgd_success)\n",
    "print(classification_report(y_val, sgd_predict, target_names=list(set(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Random Forest Classifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Naive Bayes Pipeline using FeatureUnion for our 2 predictor variables\n",
    "rf_classifier = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('title', Pipeline([('text', TextSelector('title')),\n",
    "                            ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, stop_words=stopwords.words('english')))]\n",
    "        )),\n",
    "        ('selftext', Pipeline([('text', TextSelector('selftext')),\n",
    "                               ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, stop_words=stopwords.words('english')))]\n",
    "        ))\n",
    "    ])),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = rf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Success Rate 0.49634362047572533\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      " EatingDisorders       0.80      0.04      0.08       187\n",
      "   hardshipmates       0.56      0.34      0.43      2639\n",
      "      depression       0.59      0.38      0.46      2223\n",
      "        feelgood       0.97      0.62      0.76       200\n",
      "      alcoholism       0.57      0.10      0.16      1063\n",
      "   BipolarReddit       0.00      0.00      0.00        15\n",
      "            dpdr       0.28      0.02      0.04       315\n",
      " getting_over_it       0.45      0.50      0.47      8802\n",
      "   socialanxiety       0.83      0.40      0.54       761\n",
      "            ptsd       0.48      0.74      0.58     14731\n",
      "           Anger       0.92      0.19      0.31       429\n",
      "  rapecounseling       0.00      0.00      0.00         2\n",
      " psychoticreddit       0.00      0.00      0.00       315\n",
      "            MMFB       0.00      0.00      0.00        32\n",
      "    StopSelfHarm       0.39      0.04      0.07      1179\n",
      "      PanicParty       0.00      0.00      0.00        15\n",
      "    SuicideWatch       0.76      0.21      0.33       710\n",
      "    mentalhealth       0.69      0.14      0.23       446\n",
      "   traumatoolbox       0.72      0.26      0.38      1697\n",
      "survivorsofabuse       0.74      0.24      0.36      1989\n",
      "             BPD       0.00      0.00      0.00       126\n",
      "   schizophrenia       0.00      0.00      0.00         3\n",
      "\n",
      "       micro avg       0.50      0.50      0.50     37879\n",
      "       macro avg       0.44      0.19      0.24     37879\n",
      "    weighted avg       0.53      0.50      0.46     37879\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_predict = rf_classifier.predict(X_val)\n",
    "rf_success = np.mean(rf_predict == y_val)\n",
    "print('Random Forest Classifier Success Rate', rf_success)\n",
    "print(classification_report(y_val, rf_predict, target_names=list(set(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our model by using the FeatureUnion feature of the Pipeline function. This allows us to train our NLP multi-classification model with more than 1 predictor variable. We eliminate stop words because these words typically add no numerical meaning to a sentence. Furthermore, we stem our sentences/words (Snowball Stemming) to add text normalization to our model. This helps our model get to the root of a words without getting confused by complex prefixes or sufixes.\n",
    "\n",
    "Breaking down the specific models, it appears that the Naive Bayes model predicts very accurately for some of the subreddits, but extremely poorly for others. The other two models are more centered, with predictions ranging around 0.3-0.8.\n",
    "\n",
    "To look at the accuracy of our models, we simply look at our trained models and apply them to the validation set. We look at the ratio of the amount of correct responses vs the total amount of responses. As you can see above, SGD had the highest validation accuracy = 0.6502811584255128. Therefore, we will be choosing the **SGD model** as our model of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Re-Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The only two other columns that may be useful for are 'num_comments' and 'subreddit_type'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to change the subreddit_type column from public=0 and restricted=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**New Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['restricted', 'public', None]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Different subreddit types\n",
    "list(set(d['subreddit_type'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have to remove all of the rows that have *None* for subreddit type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many None types are there 216138\n"
     ]
    }
   ],
   "source": [
    "print('how many None types are there', len(d[d.subreddit_type.isin([None])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this value is so large (there would only be ~20k data points left), we are not going to include the subreddit_type variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**New Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = d[['title', 'selftext', 'num_comments']]\n",
    "y = d['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5) # random_state is for reproducibility\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X variables:\n",
    "1. title\n",
    "2. selftext\n",
    "3. num_comments\n",
    "\n",
    "Y variables:\n",
    "1. subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the same 3 models that we used before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Naive Bayes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Naive Bayes Pipeline using FeatureUnion for our 4 predictor variables\n",
    "nb_classifier_mod = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('title', Pipeline([('text', TextSelector('title')),\n",
    "                            ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, stop_words=stopwords.words('english')))]\n",
    "        )),\n",
    "        ('selftext', Pipeline([('text', TextSelector('selftext')),\n",
    "                               ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, stop_words=stopwords.words('english')))]\n",
    "        )),\n",
    "        ('num_comments', Pipeline([('num', NumberSelector('num_comments'))]\n",
    "        ))\n",
    "    ])),\n",
    "    ('clf', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier_mod = nb_classifier_mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Modified Success Rate 0.45745663824282584\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      " EatingDisorders       0.00      0.00      0.00       187\n",
      "   hardshipmates       0.88      0.10      0.18      2639\n",
      "      depression       0.76      0.12      0.21      2223\n",
      "        feelgood       0.00      0.00      0.00       200\n",
      "      alcoholism       0.00      0.00      0.00      1063\n",
      "   BipolarReddit       0.00      0.00      0.00        15\n",
      "            dpdr       0.00      0.00      0.00       315\n",
      " getting_over_it       0.63      0.35      0.45      8802\n",
      "   socialanxiety       1.00      0.00      0.01       761\n",
      "            ptsd       0.42      0.92      0.58     14731\n",
      "           Anger       0.00      0.00      0.00       429\n",
      "  rapecounseling       0.00      0.00      0.00         2\n",
      " psychoticreddit       0.00      0.00      0.00       315\n",
      "            MMFB       0.00      0.00      0.00        32\n",
      "    StopSelfHarm       0.00      0.00      0.00      1179\n",
      "      PanicParty       0.00      0.00      0.00        15\n",
      "    SuicideWatch       0.00      0.00      0.00       710\n",
      "    mentalhealth       0.00      0.00      0.00       446\n",
      "   traumatoolbox       0.89      0.04      0.07      1697\n",
      "survivorsofabuse       1.00      0.02      0.03      1989\n",
      "             BPD       0.00      0.00      0.00       126\n",
      "   schizophrenia       0.00      0.00      0.00         3\n",
      "\n",
      "       micro avg       0.46      0.46      0.46     37879\n",
      "       macro avg       0.25      0.07      0.07     37879\n",
      "    weighted avg       0.53      0.46      0.36     37879\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_predict_mod = nb_classifier_mod.predict(X_val)\n",
    "nb_success_mod = np.mean(nb_predict_mod == y_val)\n",
    "print('Naive Bayes Modified Success Rate', nb_success_mod)\n",
    "print(classification_report(y_val, nb_predict_mod, target_names=list(set(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stochastic Gradient Descent Classification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Stochastic Gradient Descent Classification Pipeline using FeatureUnion for our 2 predictor variables\n",
    "sgd_classifier_mod = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('title', Pipeline([('text', TextSelector('title')),\n",
    "                            ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, stop_words=stopwords.words('english')))]\n",
    "        )),\n",
    "        ('selftext', Pipeline([('text', TextSelector('selftext')),\n",
    "                               ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, stop_words=stopwords.words('english')))]\n",
    "        )),\n",
    "        ('num_comments', Pipeline([('num', NumberSelector('num_comments'))]\n",
    "        ))\n",
    "    ])),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, max_iter=5, random_state=5)) # Ridge Regression\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_classifier_mod = sgd_classifier_mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent Classification Modified Success Rate 0.4296047942131524\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      " EatingDisorders       0.79      0.08      0.15       187\n",
      "   hardshipmates       0.94      0.31      0.46      2639\n",
      "      depression       0.84      0.21      0.33      2223\n",
      "        feelgood       0.85      0.45      0.59       200\n",
      "      alcoholism       0.87      0.07      0.13      1063\n",
      "   BipolarReddit       0.00      0.00      0.00        15\n",
      "            dpdr       0.52      0.03      0.07       315\n",
      " getting_over_it       0.31      0.97      0.46      8802\n",
      "   socialanxiety       0.74      0.51      0.61       761\n",
      "            ptsd       0.81      0.28      0.42     14731\n",
      "           Anger       0.98      0.18      0.31       429\n",
      "  rapecounseling       0.00      0.00      0.00         2\n",
      " psychoticreddit       1.00      0.00      0.01       315\n",
      "            MMFB       0.00      0.00      0.00        32\n",
      "    StopSelfHarm       0.42      0.07      0.13      1179\n",
      "      PanicParty       0.00      0.00      0.00        15\n",
      "    SuicideWatch       0.90      0.21      0.34       710\n",
      "    mentalhealth       0.85      0.17      0.28       446\n",
      "   traumatoolbox       0.56      0.43      0.48      1697\n",
      "survivorsofabuse       0.74      0.32      0.44      1989\n",
      "             BPD       0.40      0.02      0.03       126\n",
      "   schizophrenia       0.00      0.00      0.00         3\n",
      "\n",
      "       micro avg       0.43      0.43      0.43     37879\n",
      "       macro avg       0.57      0.20      0.24     37879\n",
      "    weighted avg       0.68      0.43      0.40     37879\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd_predict_mod = sgd_classifier_mod.predict(X_val)\n",
    "sgd_success_mod = np.mean(sgd_predict_mod == y_val)\n",
    "print('Stochastic Gradient Descent Classification Modified Success Rate', sgd_success_mod)\n",
    "print(classification_report(y_val, sgd_predict_mod, target_names=list(set(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Random Forest Classifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Naive Bayes Pipeline using FeatureUnion for our 2 predictor variables\n",
    "rf_classifier_mod = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('title', Pipeline([('text', TextSelector('title')),\n",
    "                            ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, stop_words=stopwords.words('english')))]\n",
    "        )),\n",
    "        ('selftext', Pipeline([('text', TextSelector('selftext')),\n",
    "                               ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, stop_words=stopwords.words('english')))]\n",
    "        )),\n",
    "        ('num_comments', Pipeline([('num', NumberSelector('num_comments'))]\n",
    "        ))\n",
    "    ])),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier_mod = rf_classifier_mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Modified Success Rate 0.5095435465561393\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      " EatingDisorders       0.76      0.12      0.20       187\n",
      "   hardshipmates       0.58      0.39      0.47      2639\n",
      "      depression       0.60      0.37      0.46      2223\n",
      "        feelgood       0.92      0.39      0.55       200\n",
      "      alcoholism       0.57      0.11      0.18      1063\n",
      "   BipolarReddit       0.00      0.00      0.00        15\n",
      "            dpdr       0.50      0.06      0.11       315\n",
      " getting_over_it       0.45      0.52      0.48      8802\n",
      "   socialanxiety       0.81      0.44      0.57       761\n",
      "            ptsd       0.50      0.74      0.59     14731\n",
      "           Anger       0.97      0.23      0.37       429\n",
      "  rapecounseling       0.00      0.00      0.00         2\n",
      " psychoticreddit       0.14      0.00      0.01       315\n",
      "            MMFB       0.00      0.00      0.00        32\n",
      "    StopSelfHarm       0.27      0.02      0.03      1179\n",
      "      PanicParty       0.00      0.00      0.00        15\n",
      "    SuicideWatch       0.79      0.29      0.43       710\n",
      "    mentalhealth       0.67      0.16      0.25       446\n",
      "   traumatoolbox       0.73      0.28      0.41      1697\n",
      "survivorsofabuse       0.73      0.28      0.40      1989\n",
      "             BPD       0.29      0.02      0.03       126\n",
      "   schizophrenia       0.00      0.00      0.00         3\n",
      "\n",
      "       micro avg       0.51      0.51      0.51     37879\n",
      "       macro avg       0.47      0.20      0.25     37879\n",
      "    weighted avg       0.53      0.51      0.48     37879\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_predict_mod = rf_classifier_mod.predict(X_val)\n",
    "rf_success_mod = np.mean(rf_predict_mod == y_val)\n",
    "print('Random Forest Classifier Modified Success Rate', rf_success_mod)\n",
    "print(classification_report(y_val, rf_predict_mod, target_names=list(set(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NB</th>\n",
       "      <th>SGD</th>\n",
       "      <th>RF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>old</th>\n",
       "      <td>0.4672</td>\n",
       "      <td>0.6502</td>\n",
       "      <td>0.4921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>0.4574</td>\n",
       "      <td>0.4296</td>\n",
       "      <td>0.5047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         NB     SGD      RF\n",
       "old  0.4672  0.6502  0.4921\n",
       "new  0.4574  0.4296  0.5047"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparison between the old and new models\n",
    "pd.DataFrame([[0.4672, 0.6502, 0.4921], [0.4574, 0.4296, 0.5047]], index=['old', 'new'], columns=['NB', 'SGD', 'RF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the modified models we just created we would choose the Random forest model because it has sthe highest validation score. However, it is also worth noting that the Random forest model is the only model that increased with the addition of the num_comments column. The Stochastic Gradient Descent Model's accuracy decreased significantly with the addition of the num_comments column. Overall, it seems that we are better off only using the two variables and sticking with the original SGD model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-sample accuracy for the original SGD model using the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent Classification Final Success Rate 0.6479756700247101\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      " EatingDisorders       0.71      0.29      0.41       245\n",
      "   hardshipmates       0.86      0.54      0.66      3341\n",
      "      depression       0.78      0.53      0.63      2811\n",
      "        feelgood       0.83      0.90      0.87       240\n",
      "      alcoholism       0.92      0.15      0.26      1297\n",
      "   BipolarReddit       0.20      0.05      0.08        19\n",
      "            dpdr       0.71      0.21      0.33       400\n",
      " getting_over_it       0.61      0.71      0.65     10984\n",
      "   socialanxiety       0.76      0.88      0.81      1052\n",
      "            ptsd       0.61      0.78      0.68     18290\n",
      "           Anger       0.92      0.55      0.69       550\n",
      "  rapecounseling       0.00      0.00      0.00         4\n",
      " psychoticreddit       0.00      0.00      0.00       396\n",
      "            MMFB       0.75      0.08      0.14        38\n",
      "    StopSelfHarm       0.50      0.04      0.07      1539\n",
      "      PanicParty       0.00      0.00      0.00        25\n",
      "    SuicideWatch       0.80      0.55      0.65       865\n",
      "    mentalhealth       0.76      0.65      0.70       589\n",
      "   traumatoolbox       0.76      0.56      0.65      2065\n",
      "survivorsofabuse       0.71      0.58      0.64      2472\n",
      "             BPD       0.62      0.08      0.14       125\n",
      "   schizophrenia       0.00      0.00      0.00         2\n",
      "\n",
      "       micro avg       0.65      0.65      0.65     47349\n",
      "       macro avg       0.58      0.37      0.41     47349\n",
      "    weighted avg       0.66      0.65      0.63     47349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd_predict_final = sgd_classifier.predict(X_test)\n",
    "sgd_success_final = np.mean(sgd_predict_final == y_test)\n",
    "print('Stochastic Gradient Descent Classification Final Success Rate', sgd_success_final)\n",
    "print(classification_report(y_test, sgd_predict_final, target_names=list(set(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use more sophisticated models such as neural networks\n",
    "2. Use a pre-trained model such as word_2_vec\n",
    "3. Analyze the data in a deeper way\n",
    "4. Use sentence sentiment relative to other similar sentences\n",
    "5. Use parameter grid search - I tried this, but it was too computationally expensive and could lead to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the explanation can be found above. In the end, the original **SGD** model with only the title and selftext variables is the best."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
